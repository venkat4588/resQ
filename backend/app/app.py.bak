from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
import cv2
import numpy as np
import json
import asyncio
import os
import uuid
from datetime import datetime
from typing import Dict, Set, Deque, Optional
from collections import deque
import base64
import traceback
from pathlib import Path

from .severity import predict_severity         # NEW
from .video_utils import compute_deformation, estimate_speed   # NEW
from .pose_utils import analyze_pose           # NEW

from Nirikshan.pipeline.training_pipeline import TrainingPipeline
from Nirikshan.logger import logging
import supervision as sv

# -------------------------------------
BASE_DIR = Path(__file__).resolve().parent
ACCIDENT_IMAGES_DIR = BASE_DIR / "accident_images"
ACCIDENT_IMAGES_DIR.mkdir(exist_ok=True)

PUBLIC_IMAGES_DIR = BASE_DIR.parent / "frontend" / "public" / "accident_images"
PUBLIC_IMAGES_DIR.mkdir(exist_ok=True, parents=True)

app = FastAPI(title="Nirikshan â€” Accident Detection API")
app.mount("/accident_images", StaticFiles(directory=str(ACCIDENT_IMAGES_DIR)), name="accident_images")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

pipeline = TrainingPipeline()
CONFIDENCE_THRESHOLD = getattr(pipeline, "CONFIDENCE_THRESHOLD", getattr(pipeline, "confidence_threshold", 0.4))

active_connections: Dict[str, WebSocket] = {}
detected_accidents: Dict[str, Set[int]] = {}
frame_buffers: Dict[str, Deque] = {}
tracker_instances: Dict[str, sv.ByteTrack] = {}
traces: Dict[str, Dict[int, deque]] = {}
cctv_metadata: Dict[str, Dict] = {}

MIN_FRAMES_BETWEEN_DETECTIONS = 30
BUFFER_SIZE = 15
POST_ACCIDENT_FRAMES = 15
ACCIDENT_COOLDOWN_FRAMES = 90
ACCIDENT_STATE_DURATION = 120
TRACE_LENGTH = 30
MAX_TRACE_POINTS = 90

CLASS_NAMES = {
    0: "bike",
    1: "bike_bike_accident",
    2: "bike_object_accident",
    3: "bike_person_accident",
    4: "car",
    5: "car_bike_accident",
    6: "car_car_accident",
    7: "car_object_accident",
    8: "car_person_accident",
    9: "person"
}
VEHICLE_CLASS_IDS = [0, 4]
ACCIDENT_CLASS_IDS = [1, 2, 3, 5, 6, 7, 8]


def format_location(latitude: Optional[float], longitude: Optional[float]) -> str:
    if latitude is None or longitude is None:
        return "Unknown location"
    return f"{latitude:.6f}, {longitude:.6f}"

def safe_detect_objects(frame: np.ndarray):
    try:
        if hasattr(pipeline, "model_trainer") and hasattr(pipeline.model_trainer, "detect_objects"):
            return pipeline.model_trainer.detect_objects(frame)
        if hasattr(pipeline, "detect_objects"):
            return pipeline.detect_objects(frame)
    except Exception as e:
        logging.error(f"Model detection error: {e}")
        logging.error(traceback.format_exc())
    return None, None, None

def save_accident_image(frame: np.ndarray, filename_prefix: str = "accident") -> Optional[str]:
    if frame is None:
        logging.error("save_accident_image called with None frame")
        return None
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = uuid.uuid4().hex[:8]
        filename = f"{filename_prefix}_{timestamp}_{unique_id}.jpg"
        backend_path = ACCIDENT_IMAGES_DIR / filename
        public_path = PUBLIC_IMAGES_DIR / filename
        success = cv2.imwrite(str(backend_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        if not success:
            logging.error(f"cv2.imwrite failed for {backend_path}")
            return None
        if not backend_path.exists() or backend_path.stat().st_size == 0:
            logging.error(f"Failed to create valid image file at {backend_path}")
            return None
        try:
            import shutil
            shutil.copy2(str(backend_path), str(public_path))
        except Exception:
            logging.exception("Failed to copy to public directory (non-fatal)")
        return f"/accident_images/{filename}"
    except Exception:
        logging.exception("Unexpected error saving accident image")
        return None

def base64_to_image(base64_string: str) -> Optional[np.ndarray]:
    try:
        if "base64," in base64_string:
            base64_string = base64_string.split("base64,")[1]
        imgdata = base64.b64decode(base64_string)
        nparr = np.frombuffer(imgdata, np.uint8)
        return cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    except Exception:
        logging.exception("Failed to decode base64 image")
        return None

@app.get("/")
def root():
    return {"message": "Nirikshan Backend Running"}

@app.get("/health")
async def health_check():
    return {
        "status": "ok",
        "images_dir": str(ACCIDENT_IMAGES_DIR),
        "public_dir": str(PUBLIC_IMAGES_DIR),
        "confidence_threshold": CONFIDENCE_THRESHOLD,
    }

@app.get("/images")
async def list_images():
    images = []
    for file in ACCIDENT_IMAGES_DIR.glob("*.jpg"):
        images.append({
            "filename": file.name,
            "url": f"/accident_images/{file.name}",
            "created": datetime.fromtimestamp(file.stat().st_ctime).isoformat(),
            "size_bytes": file.stat().st_size
        })
    return {"images": images, "count": len(images)}

@app.websocket("/ws/detect")
async def accident_detection_websocket(websocket: WebSocket):
    await websocket.accept()
    connection_id = f"conn_{uuid.uuid4().hex[:8]}"
    active_connections[connection_id] = websocket
    detected_accidents[connection_id] = set()
    logging.info(f"Client connected: {connection_id}")

    try:
        await websocket.send_json({"type": "ready", "message": "Backend ready for video processing", "severity": "info"})
        while True:
            message = await websocket.receive_text()
            data = json.loads(message)
            if data.get("type") == "ping":
                await websocket.send_json({"type": "pong"})
                continue
            if data.get("type") == "process_video":
                video_url = data.get("video_url")
                cctv_metadata[connection_id] = {
                    "name": data.get("camera_name", "Unknown Camera"),
                    "latitude": data.get("latitude"),
                    "longitude": data.get("longitude"),
                    "camera_id": data.get("camera_id")
                }
                if video_url:
                    await process_video_stream(websocket, video_url, connection_id)
    except WebSocketDisconnect:
        logging.info(f"Client disconnected: {connection_id}")
    except Exception:
        logging.exception("Unhandled exception in websocket")
    finally:
        active_connections.pop(connection_id, None)
        detected_accidents.pop(connection_id, None)
        traces.pop(connection_id, None)
        tracker_instances.pop(connection_id, None)
        cctv_metadata.pop(connection_id, None)
        logging.info(f"Cleaned up connection: {connection_id}")

async def process_video_stream(websocket: WebSocket, video_url: str, connection_id: str):
    try:
        frame_buffers[connection_id] = deque(maxlen=BUFFER_SIZE)
        traces[connection_id] = {}
        tracker = sv.ByteTrack(track_activation_threshold=0.25, lost_track_buffer=30, minimum_matching_threshold=0.8, frame_rate=24)
        tracker_instances[connection_id] = tracker

        frame_count = 0
        accident_found = False
        in_accident_state = False
        accident_state_frames = 0

        if video_url.startswith("/"):
            # video_url like "/uploads/<file>" saved by Next.js under frontend/public
            # BASE_DIR = <repo_root>/backend/app, so frontend public is two levels up
            video_path = str((BASE_DIR.parent.parent / "frontend" / "public" / video_url.lstrip("/")).resolve())
        else:
            video_path = video_url

        logging.info(f"[{connection_id}] Opening video from: {video_path}")
        if not os.path.exists(video_path):
            logging.error(f"[{connection_id}] Video path does not exist: {video_path}")
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise Exception(f"Could not open video file: {video_path}")

        original_fps = cap.get(cv2.CAP_PROP_FPS) or 24.0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
        target_fps = min(original_fps, 24.0)
        frame_interval = 1.0 / target_fps
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 1280)
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 720)
        scale_factor = 1.0
        if width > 1280:
            scale_factor = 1280 / width
            width = 1280
            height = int(height * scale_factor)

        await websocket.send_json({
            "type": "video_info",
            "width": width,
            "height": height,
            "fps": target_fps,
            "original_fps": original_fps,
            "total_frames": total_frames,
            "message": f"Processing video at {target_fps} FPS ({width}x{height})",
            "severity": "info"
        })

        frame_times = []
        last_frame_time = asyncio.get_event_loop().time()
        box_annotator = sv.BoxAnnotator(thickness=2)

        while cap.isOpened():
            batch_start = asyncio.get_event_loop().time()
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1
            if scale_factor < 1.0:
                frame = cv2.resize(frame, (width, height))
            frame_buffers[connection_id].append(frame.copy())
            boxes, class_ids, confidences = safe_detect_objects(frame)
            if boxes is not None and hasattr(boxes, "shape") and boxes.shape[0] > 0:
                boxes_np = np.array(boxes, dtype=np.float32)
                confidences_np = np.array(confidences, dtype=np.float32)
                class_ids_np = np.array(class_ids, dtype=np.int32)
                detections = sv.Detections(xyxy=boxes_np, confidence=confidences_np, class_id=class_ids_np)
                tracked_detections = tracker.update_with_detections(detections)
            else:
                tracked_detections = sv.Detections.empty()

            display_frame = frame.copy()
            accident_indices = []
            accident_detected = False

            if len(tracked_detections) > 0:
                for i in range(len(tracked_detections)):
                    track_id = tracked_detections.tracker_id[i]
                    if track_id is None:
                        continue
                    class_id = int(tracked_detections.class_id[i])
                    confidence = float(tracked_detections.confidence[i])
                    is_accident = class_id in ACCIDENT_CLASS_IDS
                    if is_accident and confidence >= CONFIDENCE_THRESHOLD:
                        accident_indices.append(i)
                        accident_detected = True
                    if track_id not in traces[connection_id]:
                        traces[connection_id][track_id] = deque(maxlen=MAX_TRACE_POINTS)
                    bbox = tracked_detections.xyxy[i]
                    center_x = int((bbox[0] + bbox[2]) / 2)
                    center_y = int((bbox[1] + bbox[3]) / 2)
                    traces[connection_id][track_id].append((center_x, center_y))

            # ----- NEW: Severity & Injury Prediction -----
            if accident_detected and not in_accident_state:
                in_accident_state = True
                accident_state_frames = 0
                accident_found = True
                if accident_indices:
                    idx = accident_indices[0]
                    confidence = float(tracked_detections.confidence[idx])
                    class_id = int(tracked_detections.class_id[idx])
                    class_name = CLASS_NAMES.get(class_id, "Unknown")
                    location = "Unknown location"
                    meta = cctv_metadata.get(connection_id, {})
                    if meta.get("latitude") is not None and meta.get("longitude") is not None:
                        location = format_location(meta["latitude"], meta["longitude"])

                    # --- (1) Feature extraction: deformation, speed, pose features ----
                    pre_frame = frame_buffers[connection_id][0] if len(frame_buffers[connection_id]) > 0 else display_frame
                    deformation = compute_deformation(pre_frame, display_frame)
                    speed = estimate_speed(list(frame_buffers[connection_id]))
                    pose_features = analyze_pose(display_frame)
                    features = [deformation, speed] + pose_features

                    # --- (2) Predict severity & injury ---
                    severity, injury = predict_severity(features)

                    await websocket.send_json({
                        "type": "accident",
                        "accident_detected": True,
                        "frame_number": frame_count,
                        "confidence": confidence,
                        "accident_type": class_name,
                        "location": location,
                        "severity": severity,
                        "injury_prediction": injury,
                        "message": f"{class_name} detected at frame {frame_count}",
                        "timestamp": datetime.now().timestamp()
                    })

                    # Save accident image (optionally, after adding severity info on frame)
                    image_url = save_accident_image(display_frame)
                    if image_url:
                        await websocket.send_json({
                            "type": "image_saved",
                            "message": f"Accident image saved: {image_url}",
                            "severity": "info",
                            "image_url": image_url,
                            "frame_number": frame_count,
                            "accident_type": class_name,
                            "confidence": confidence,
                            "location": location,
                            "timestamp": datetime.now().timestamp()
                        })

            if in_accident_state:
                accident_state_frames += 1
                if accident_state_frames >= ACCIDENT_STATE_DURATION:
                    in_accident_state = False

            _, buffer = cv2.imencode('.jpg', display_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
            encoded_frame = base64.b64encode(buffer).decode('utf-8')

            await websocket.send_json({
                "type": "frame",
                "frame": encoded_frame,
                "frame_number": frame_count,
                "timestamp": datetime.now().timestamp(),
                "display_time": frame_count / (original_fps or 24.0),
                "total_frames": total_frames,
                "progress": frame_count / total_frames if total_frames > 0 else 0
            })

            if frame_count % 30 == 0:
                await websocket.send_json({
                    "type": "progress",
                    "message": f"Processed {frame_count} of {total_frames} frames",
                    "severity": "info",
                    "frame_count": frame_count,
                    "progress": frame_count / total_frames if total_frames > 0 else 0
                })

            now = asyncio.get_event_loop().time()
            elapsed = now - last_frame_time
            sleep_time = max(0, frame_interval - elapsed)
            await asyncio.sleep(sleep_time)
            last_frame_time = asyncio.get_event_loop().time()

        cap.release()
        meta = cctv_metadata.get(connection_id, {})
        location = "Unknown location"
        if meta.get("latitude") is not None and meta.get("longitude") is not None:
            location = format_location(meta["latitude"], meta["longitude"])

        await websocket.send_json({
            "type": "processing_complete",
            "message": "Video processing completed",
            "severity": "info",
            "accident_found": accident_found,
            "total_frames": frame_count,
            "location": location,
            "timestamp": datetime.now().timestamp()
        })
    except Exception:
        logging.exception("Error processing video stream")
        try:
            await websocket.send_json({"type": "error", "message": "Error processing video", "severity": "error"})
        except Exception:
            logging.exception("Failed to send error message to websocket")

@app.post("/detect/image")
async def detect_image(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        nparr = np.frombuffer(contents, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        result = pipeline.process_frame(img) if hasattr(pipeline, "process_frame") else {"error": "No process_frame method"}
        return JSONResponse(content={"result": result})
    except Exception:
        logging.exception("Error in detect_image")
        return JSONResponse(status_code=500, content={"error": "Failed to process image"})

@app.post("/detect/video")
async def detect_video(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        video_path = str(BASE_DIR / "uploads" / f"videoUpload_{timestamp}.mp4")
        os.makedirs(os.path.dirname(video_path), exist_ok=True)
        with open(video_path, "wb") as f:
            f.write(contents)
        if hasattr(pipeline, "reset_state"):
            pipeline.reset_state()
        result = pipeline.process_video(video_path) if hasattr(pipeline, "process_video") else {"error": "No process_video method"}
        return JSONResponse(content={"result": result})
    except Exception:
        logging.exception("Error in detect_video")
        return JSONResponse(status_code=500, content={"error": "Failed to process video"})

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)
